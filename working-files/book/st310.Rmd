---
title: "ST310 - Machine Learning"
author: "Kamal Sacranie"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: report
bibliography: []
biblio-style: apalike
link-citations: yes
geometry: a4paper,margin=3cm
subtitle: "Machine Learning course using R with Joshua Loftus"
---

```{r, setup, include=F}
knitr::opts_chunk$set(
  attr.source = ".numberLines", eval = F, warning = F,
  message = F, cache = T
)
```

# Preface {-}

The main readings used in this course are:

- **ISLR** [Introduction to Statistical Learning](https://statlearning.com/) 
- **ESL** [Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/)
- **CASI** [Computer Age Statistical Inference](https://web.stanford.edu/~hastie/CASI/)
- **Mixtape** [Causal Inference: The Mixtape](https://mixtape.scunning.com/index.html)
- **R4DS** [R for Data Science](https://r4ds.had.co.nz/)

> In this course the textbook readings are required readings but the lecture
> video emphasise themes that complement the textbook

I Like this guy. I wish I had taken this course for real lol :(

## `R` package list

- `tidyverse`: just a bunch of commonly used packages
- `gapminder`: just some random datasets we can use

```{r, eval=T, results=F}
library(gapminder)
library(fivethirtyeight)
library(broom)
library(tidyverse)
library(MASS)
```

<!--chapter:end:index.Rmd-->

---
author: "Kamal Sacranie"
numbersections: true
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: bookdown::pdf_document2
---

# (PART) Week 1 {-}

```{r, include=F}
knitr::opts_chunk$set(echo = TRUE)
```

# Simple plotting with `ggplot`

What is the association between GDP and life-expectancy:

```{r, echo=FALSE}
library(gapminder)
library(tidyverse)
```

## Using `ggplot`

::: {.example #gdp-life-exp name="GDP vs life expectancy"}
<br />\hfill\break

::: {.minipage latex-data=""}
```{r yes, fig.cap="The cars data."}
ggplot(
  # Piping to mutate to boolean index the data
  gapminder %>% mutate(
    # Simple boolean logic here and passing it as a kwarg to the mutate
    # function
    indicator = (country == "United Kingdom")
  ),
  aes(x = gdpPercap, y = lifeExp)
) +
  # How the fuck has indicator now got local scope. Was it created by the
  # mutate fucntion. But even then? it's not returning anything, where is
  # idnicator being assigned
  geom_point(aes(color = indicator))
```
:::

:::

---

- `aes` stands for aesthetic and creates an aesthetic mapping between our
  dataset and the aesthetic properties of the plot. It's a function which is
  passed as an in place arg called a `mapping` function
- `aes` takes an x variable and y variable form our dataset as arguments
- `geom_point` is one of the many plots that come with `ggplot` and plots a
  point plot

> The `aes` argument which goes in the first `ggplot` function applier for the
> whole "canvas", whereas the ones for the secondary functions only apply to
> that specific function. So **if you want to apply colour to only the
> regression line, you do the `aes` in the `geom_smooth` function**

In the above example we use the `%>%` operator (provided by the package dyplr).

This is maybe the only good thing about R that I have encoutered. It works
similarly to the pipe character in bash. It takes the return from the last
function pipes it into the first argument of the next function.

> For the record, it is insanely retarded that we can just pass through the
> columns of `gapminder` as if they were variables. We haven't assigned them or
> anything. It's also insane to me that we use ggplot and then ADD A PLOT?? WTF
> does that even mean. Why are these two objects able to be added. How is there
> not a type error

## The pipe `%>%`

As previously mentioned... the only good thing about `R`. Just plugs the data
into the first argument of the next function.

::: {.example #pipe-vs-normal name="Piping to a function"}
<br />\hfill\break

```{r}
gapminder %>% nrow()
```

is the same as

```{r}
nrow(gapminder)
```

:::

---

This is often what `R` uses to stack things. It's basically becasue `R` is made
by people who are too retarded to read regular nested code or even just regular
lines of code.

## Filtering for a value in column (basically boolean indexing)

```{r}
# notice that we can pass a straight int here. R knows the type in the column
base_plot <- ggplot(
  # Easier to see here that mutate basically adds a column with a boolean value
  # showing whether it's the UK or not
  filter(gapminder, year == 2007) %>%
    mutate(indicator = (country == "United Kingdom")),
  aes(x = gdpPercap, y = lifeExp)
) + # The plus is similar to the pipe, it is introduces by ggplot and basically
  # allows you to build layers on your plot
  geom_point(aes(color = indicator))
```

- The `+` is like ggplot's version of the pipe. It's used to layer graphs
- `indicator` is accessable because it's a column of our `gapminder` data which
  is being pushed through with our plus

```{r}
base_plot +
  geom_smooth()
```

- The line curves around the points and fits locally to the points
- The gray area is the confidence intreval of the fit line

## Model objects

Everything is an object in `R` and our assignment operator is `<-`:

```{r}
gm_2007 <- gapminder %>%
  filter(year == 2007)
```

Great so we can use that everywhere in this `R` file. For example, we can use
the `lm` function for fitting models

::: {.example #lm-basic-fit name="A basic model function use"}
<br />\hfill\break

```{r}
gm_lm <- lm(lifeExp ~ gdpPercap, data = gm_2007)
```

:::

---

> A common way for model funciton to work in `R` is to require the assignment
> of the lefthand side and the right hand side variables; we see this in
> \@ref(exm:lm-basic-fit) with the use of the `~`. On the left of the tilde we
> have the dependent (outcome) variable and on the right we have the
> independent (predictor) variable

In \@ref(exm:lm-basic-fit) `lm` stands for linear model (a regression) and is
an object which contains all the information for our regression. When you print
it on its ones, there isn't much you can do

```{r}
summary(gm_lm)
residuals(gm_lm) # the difference in the predicted values and the actual
predict(gm_lm)
```

- `summary` is a builtin function which displays a summary **of any model
  object in `R`**
- This also shows us standard errors, p-values, etc.

> Something I find weird here is that all the data we need is inside the `lm`
> object but instead of using a method to produce the result like I would
> expect, we have to pass it to another function

## `broom`

`broom` is just a library that provides nice out of the box rendering for `R`'s
model objects.

```{r}
library(broom)
```

```{r}
tidy(gm_lm)
```

- The `broom::tidy` function is really nice for making tables look good
- Don't think it works in latex though

We also have the `broom::glance` function which shows our most important values
at a glance. You can also compare two models with it:

```{r}
glance(gm_lm)
```

The last main fucntion from `broom` is `augment`. This gives us the original
dataset back plus the extra rows like standard errors for residuals, residuals,
etc. This function is really useful for plotting the results of the model

```{r}
augment(gm_lm) %>%
  ggplot(aes(x = gdpPercap, y = lifeExp)) +
  geom_point() +
  # the geom line adds a line to the plot on top of the already existing plot,
  # i.e. we can give it a different x and y to plot via the aes which returns a
  # mapping
  geom_line(aes(x = gdpPercap, y = .fitted))
```

- Note here how ggplot basically crease the canvas for the plot. Then you layer
  on a point distribution using the data, then you layer on a line using the
  data and specifying the variable you want. In this case `.fitted` will give
  us all the points on the fitted line. This is the same as using ggplot's
  `geom_smooth` function. We are just doing it the long way around

## Subbing out function for e.g. `loess`

The most important takeaway here is the idea of **model flexibility**. We have
made a robust programme which let's us swap in and out functions. For example,
let's use the `loess` function to plot the local extimation line which is the
default given by ggplot's `geom_smooth`

```{r}
gm_loess <- loess(lifeExp ~ gdpPercap, data = gm_2007, span = .5)
summary(gm_loess)
```

- Again, we see the tilde notation for specifying LHS and RHS
- An interesting note. When I ran what I thought was known as the `tidy`
  **function**, `R` threw an error mentioning loess object doesn't have the
  method `tidy`. I.e. methods are defined in objects but somehow they are
  called with function notation.

::: {.example #loess name="loess example"}
<br />\hfill\break

```{r}
augment(gm_loess) %>%
  ggplot(aes(gdpPercap, lifeExp)) +
  geom_point() +
  geom_line(aes(gdpPercap, .fitted))
```

:::

---

### What is `loess` actaully doing

`loess` stands for "locally estimated scatter-plot smoothing". If we look at
the output of exmaple \@ref(exm:loess), we see that it is looking at the local
points and approximating them. `loess` has some options, for example:

- `span`: tells the function how localised to make its approximation

<!--chapter:end:01-introduction-to-r.Rmd-->

---
author: "Kamal Sacranie"
numbersections: true
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: bookdown::pdf_document2
---

# Candy rankings

```{r, include=F}
library(plotly)
library(broom)
library(fivethirtyeight)
library(tidyverse)
```

It's always good practice to start by visualising some of your data. For fun
let's just fit a simple linear model to this dataset:

```{r}
cr_lm <- lm(pricepercent ~ winpercent, data = candy_rankings)
tidy(cr_lm)
```

```{r}
augment(cr_lm) %>%
  ggplot(aes(x = pricepercent, y = winpercent)) +
  geom_point() +
  geom_line(aes(y = .fitted))
```

- Nice tip with `geom` functions is that because it take in the object the
  previous function returns, you can keep the `x` value the same and reassign
  the `y`

## Adding a categorical variable to our model

```{r}
cr_lm_cat <- lm(winpercent ~ pricepercent + chocolate, candy_rankings)
tidy(cr_lm_cat)
```

- Here in the `formula` argument, we add the categorical variable `chocolate`
- This becomes a dummy variable for our regression model (`chocolateTRUE`)

Let's now represent this new categorical variable on our plot through the use
of colour:

```{r}
augment(cr_lm_cat) %>%
  ggplot(aes(
    x = pricepercent,
    y = winpercent,
    color = chocolate,
    shape = chocolate,
    linetype = chocolate
  )) +
  geom_point() +
  geom_line(aes(y = .fitted))
```

- So in the `aes` function we have a whole load of arguments which take a set
  of true or false values to boolean index the characteristics of the data

## Adding a continuous predictor and creating a 3D plot

A common package used to create 3D plots is `plotly`. 

```{r}
candy3d <- plotly::plot_ly(candy_rankings,
  x = ~pricepercent, y = ~sugarpercent,
  z = ~winpercent,
  type = "scatter3d"
)
```

> This is a perfect example of how `R` is so good out the box for data
> visualisation

We can fit our regression plane in this three dimensional space:

```{r}
cr_lm_sugar <- lm(
  winpercent ~ pricepercent + sugarpercent,
  data = candy_rankings
)

xy_plane <- expand.grid(0:100, 0:100) / 100
ps_plane <- xy_plane %>%
  rename(pricepercent = Var1, sugarpercent = Var2)

lm_plane <- augment(cr_lm_sugar, newdata = ps_plane)
lm_matrix <- matrix(lm_plane$.fitted, nrow = 101, ncol = 101)

candy3d %>%
  add_surface(
    x = ~ (0:100) / 100,
    y = ~ (0:100) / 100,
    z = ~lm_matrix
  )
```

- The `expand.grid` function creates a data frame from all combinations of the
  supplied vectors or factors
- We then rename the variable of the plane with `rename`
- We then see the `augment` function in use 


<!--chapter:end:02-more-involved-models.Rmd-->

# Machine learning applications

Some examples of machine learning are:

- Scraping social-media to predict period of unrest before it happens
- Using health records to predict which patients will require more care
- Algorithms which collect data to adapt

Basically any predictive model that uses data to predict the future. In this
course we focus on the underlying theory of how the methods function.

> The difference between AI and ML, is that AI allows us to get mathematical
> relationships from data that is unstructure, non-mathematical. This could be
> images etc. Contrasting this with what we did in section
> \@ref(simple-plotting-with-ggplot) where our inputs were very mathematical
> and numerical. In this course **when we refer to data, we are already talking
> about vectorised, structured data**

## Abstraction and notation

In an abstract sense, the data we look at is formatted as a collection of $p$
district **variables**^[Think columns of your data]:

\begin{align*}
  X = (X_1, X_2, \ldots, X_p) \in \mathbb{R^{p}}
\end{align*}

- We assume each **observation is a point in the vectors space**
  $\mathbb{R^{p}}$
- We also assume that $p$ is finite

**Supervised learning**: if we think of an application for ML which has a clear
$Y$ variable defined, i.e. the outcomes^[Sometimes called responsive] of the
model, the we are dealing with supervised learning.

**Unsupervised learning**: if there is no natural outcome variable $Y$, then we
are dealing with unsupervised learning. This is where you try and create a
mathematical relationship out of unstructured data.

### Sub categories of supervised ML

Common cases:

- If $Y$ is numeric: **regression**
- If $Y$ is categorical: **classification**

Special cases:

- $Y$ is binary with rare cases, e.g. anomaly detection
- $Y$ is a time to event, survival analysis
- Multi-class, hierarchical classes, etc

## How to predict $Y$ from $X$

- We want to estimate $\exists f$ such that the graph of the function $y =
  f(x)$ fit the data perfectly^[Fundamental idea of regression]
- **Problem**: what if $(x_1, y_1) = (1, 0)$ and $(x_2, y_2) = (1, 1)$?
- **Problem**: even our most tested and verified physical laws won't fit data
  perfectly

Even if we are thinking about cases where our data is supposed to follow a
well-defined law, upon measuring we will often see that this doesn't quite
work.  
The **solution** to this problem is to have an error term. For any function
$f$, we can always get the residuals:

\begin{align*}
  \epsilon\equiv y - f(x)
\end{align*}

- $y$ is the actual observation and we deduct the predicted value to see our
  residual (error)

***We want a function which minimises error.***

The simplest way to minimise this error is to find out which function decreases
our squared error^[This is premised on the fact that $\epsilon$ has some sort
of probability distribution so we can predict it]. A good function is defined
as:

\begin{align*}
  \mathbb{E}[\epsilon^2] = \mathbb{E}\{[Y - f(X)]^2\}
\end{align*}

This motivates the **plug-in principle**: compute an *estimate* $\hat{f}$ of
the good function $f$ by solving the corresponding problem on the dataset,
i.e.:

\begin{align*}
  \min\sum_{i = 1}^{n}\left[y_{i} - \hat{f}(s_{i})\right]^{2}
\end{align*}

> Basically, there is this perfect function out there $f$. It is almost
> impossible to arrive at that function so we have to calculate $\hat{f}$ which
> minimises our error and approximates our perfect function.

## Bias-variance trade-off

Errors that are systematic are bias in the system. The bias-variance trade-off
is formally described as follows:

\begin{equation}
  \mathbb{E}\{[Y - \hat{f}(X)]^{2}\} = \text{Var}(\hat{f}) +
  \text{Bias}(\hat{f})^{2} (\#eq:bias-var-to)
\end{equation}

- The expected error of our function is a combination of bias and variance

Combining this idea of BV trad-off with our idea of model complexity, we
typically see that **more complex models have lower bias and higher variance**.

Typical there is a sweet spot for complexity

Figure \@ref(fig:complexity-variance) shows us a great preview between a more
complex model which visually even has more variance and a less complex model
which doesn't deviate too much but as such has more bias.

```{r complexity-variance, echo=FALSE, eval=T}
#| out.width="40%",
#| fig.cap='complexity-variance',
#| fig.show='hold',
#| fig.align="center"
knitr::include_graphics(
  c(
    "./assets/03-machine-learning-applications/img/2022-01-23-02-58-55.png",
    "./assets/03-machine-learning-applications/img/2022-01-23-02-59-17.png"
  )
)
```

- It is difficult to tell visually which model is better

## Evaluation: mean squared error

It is extremely easy to evaluate MSE in `R`. You use the `residuals` function
which takes a `model` object as an argument:

```{r}
c(
  mean(residuals(gm_simple)^2),
  mean(residuals(gm_complex)^2)
)
```

`candy_rankings` models:

```{r}
c(
  mean(residuals(candy_simple)^2),
  mean(residuals(candy_complex)^2)
)
```

- These variables we are passing through are object from a model function like
  `lm`
- The above results would show us that the more complex models have lower MSE.
  This is mostly true because your function better approximates the data. But
  this is where the question of overfitting comes into play because if we use
  that same function on a new set of data, we could get completely wrong
  results

<!--chapter:end:03-machine-learning-applications.Rmd-->

# Seminar 1

This seminar is insanely basic. Probably nothing to write down.

<!--chapter:end:04-seminar-1.Rmd-->

# (PART) Week 2 {-}

# Regression part I

```{r, include=F}
library(tidyverse)
library(gapminder)
library(broom)
```

## Estimation

We begin by fitting our data to our standard linear regression model using
`lm`:

```{r gapminder-scatter, fig.cap="GGPlot point plot", eval=T}
gm2007 <- filter(gapminder, year == 2007)
gm_lm <- lm(lifeExp ~ gdpPercap, data = gm2007)
print(gm_lm)
ggplot(gm2007, aes(x = gdpPercap, y = lifeExp)) +
  geom_point() +
  geom_smooth(method = "loess")
```

### Regression breakdown

However, let's break down what the `lm` function does under the hood. The basic
regression model is:

\begin{equation}
  \hat{\beta}_{1} = \text{cov}(x,y)\dfrac{\sigma_{y}\sigma_{x}}{\sigma_x}
  (\#eq:beta)
\end{equation}

- $y$ goes in the numerator because if you outcome variable *has a larger
  spread*^[i.e. a higher standard deviation], then the slop has to be steeper
- $x$ is in the denominator because the horizontal axis variance leads to the
  line being less steep

We can calculate equation \@ref(eq:beta) by hand in $R$ as follows^[This is
also a "tidyverse way" of calculating statistics]. First the regular way:

```{r}
cor(gm2007$gdpPercap, gm2007$lifeExp)
sd(gm2007$gdpPercap)
```

- We see that we have to pass in our column for every summary statistic

`tidyverse` provides function which lets us only pass the data through one and
then essentially makes each column into a variable the function can access. 
Basically syntactical sugar:

```{r}
summarize(gm2007,
  cor_xy = cor(gdpPercap, lifeExp),
  sd_x = sd(gdpPercap),
  sd_y = sd(lifeExp),
  hat_beta1 = cor_xy * sd_y / sd_x
)
```

- The usefulness of summarise is that you can do this for any function applied
  to your varaibles is that you can do this for any function applied to your
  varaibles
- `summarize` basically creates those variables which we passed in as kwargs
- Note that we can just calculate beta there and then within the function using
  the previously defined variables in the call of the function
- `hat_beta1` is the same slope as calculated by `lm`

To calculate the intercept, we know that the regression line passes through

\begin{align*}
  (\overline{x},\overline{y})
\end{align*}

- i.e. the line passes through the point that is the **mean of the dataset**

which means that $\overline{y}$ is calculated with the estimated slope,
intercept and $\overline{x}$. So we can rearrange for $\hat{\beta}_0$:

\begin{equation}
  \begin{aligned}
    \overline{y}&= \hat{\beta}_0 + \hat{\beta}_1 \overline{x}\\
    \hat{\beta}_0&= \overline{y} - \hat{\beta}_1 \overline{x} (\#eq:intercept)
  \end{aligned}
\end{equation}

We can of course do this again with the `summarise` function:

```{r}
summarise(gm2007,
  cor_xy = cor(gdpPercap, lifeExp),
  sd_x = sd(gdpPercap),
  sd_y = sd(lifeExp),
  hat_beta1 = cor_xy * sd_y / sd_x,
  xbar = mean(gdpPercap),
  ybar = mean(lifeExp),
  hat_beta0 = ybar - hat_beta1 * xbar
)
```

> You should be able to reproduce these basic regression formulas by heart

We can the give a numerical value of certainty ($p$-value) which tells us how
certain we are that our model is correct. This is done by first calculating the
standard errors of the slop $\hat{\beta}_{1}$:

\begin{equation}
  \begin{aligned}
    \text{SE}(\hat \beta_1) = \sqrt{\frac{\sigma^2}{\sum_{}((x_i - \bar x)^2)}}\\
    \text{se}(\hat \beta_1) = \frac{\hat \sigma}{\sqrt{\sum_{}((x_i - \bar
    x)^2)}}
    (\#eq:se-beta1)
  \end{aligned}
\end{equation}

- The $\sigma^{2}$ here represents the variance of the $\epsilon$s in the model
  which are irreducible
- **If there is more variance in our errors** then the standard error
  increases^[You can think of this as how far
  away the points are from our regression line]
- The denominator here is the **how much spread there is in the $x$ variable**.
  Think of "how much information there is in the $x$ variable". If the $x$
  values are all very close to each other **there is lower variance**^[In this
  instance, information is the recirical of variance here] which means there is
  less information in our $x$ variable so our standard error increases
- I like to think of it like a standardised proportion of $\epsilon$ variance
  to how much variance there is in our $x$ variable^[Like, if there is low
  variance in our $x$ we would need a low variance in our errors in order for
  the estimated coefficient to be a good predictor]

If we look at the summary of the linear model which was produced by
`R`^[rendered here with the `tidy` function instead of `summary` because it
looks nicer]

```{r}
tidy(gm_lm)
```

- The value we are looking at the estimated coefficients and their standard
  errors. We want to see if we can calcualate, using equation
  \@ref(eq:se-beta1), the standard error of the estimated coefficient^[In the
  column `estimate`]

We need to estimate $\sigma^{2}_{\epsilon}$ by using the standard error of the
residuals (RSE)^[This is because the errors are not continuous data and we only
have a finite number of points so we need an estimator]:

\begin{align}
  \hat \sigma = \text{RSE} = \sqrt{\frac{\text{RSS}}{n-2}}
  (\#eq:rse)
\end{align}

- Where the RSS is the sum of the squared differences between our model fit
  line and the points?

We can calculate the RSS via the `broom` packages function `augment` which adds
the residual for each element. We can then sum all of those squared residuals
to get RSS to calculate our $\text{ RSE } = \hat{\sigma}$ using equation
\@ref(eq:rse):

```{r}
augment(gm_lm) %>%
  summarise(
    RSS = sum(.resid^2), # Summing all the squared residuals
    RSE = sqrt(RSS / (n() - 2)),
    # Calculating standar error
    std.err = RSE / sqrt(sum((gdpPercap - mean(gdpPercap))^2))
  )
```

- The function `n` here calculates the sample size of our group

### Regressing over the whole dataset and not just the `gm2007` subset using `group_by`

We can use the entire dataset and **group by a given variable** with the
following:

```{r, eval=T}
gapminder %>%
  group_by(year) %>%
  summarise(
    cor_xy = cor(gdpPercap, lifeExp),
    sd_x = sd(gdpPercap),
    sd_y = sd(lifeExp),
    hat_beta1 = cor_xy * sd_y / sd_x,
    xbar = mean(gdpPercap),
    ybar = mean(lifeExp),
    hat_beta0 = ybar - hat_beta1 * xbar
  ) %>%
  knitr::kable(booktabs = T)
```

- `group_by` basically slices and dices our dataset into the group we provide
  and then the summarise allows us to run the summary statistics on each one of
  those groups
- The output here shows us the `hat_beta1` isn't very stable over time

## Model diagnostics

Diagnostics for regression include $R^{2}$ and the RSE.

### $R^{2}$

#### Simple linear regression $R^{2}$

The `glance` function from the `broom` package outputs the $R^{2}$ and adjusted
$R^{2}$ for a model:

```{r, eval=F}
glance(gm_lm)
```

If we want to calculate this ourself, we need to use the $R^{2}$ calculation
for a SIMPLE linear regression^[One intercept and one dependent variable:
$Y=\beta_0+beta_1x_1$] which is:

\begin{align*}
  R^{2} = \text{cor}(x,y)^{2}
\end{align*}

- I.e. it's just the correlation of our two variables squared. This is only in
  the case of a SIMPLE REGRESSION

We can easily calculate this with `R`:

```{r}
cor(gm2007$gdpPercap, gm2007$lifeExp)^2
```

#### General linear regression $R^{2}$

The general formula for $R^{2}$ is:

\begin{align}
  R^{2} = 1 - \dfrac{RSS}{TSS}
\end{align}

- That is, $R^{2}$ is the proportion of the RSS to the TSS^[Total sum of
  squares].

> We commonly call this the explanation power of our model. It is the
> proportion of our sum of squares of residuals to the total sum of squares of
> our $Y$ variable, i.e. all the values less their means (i.e. the "centred"
> values) squared and the summed up
>
> \begin{align*}
    TSS = \sum(y - \overline{y})^{2})
  \end{align*}

In `R` we can just calculate this with `tidyverse::augment`^[Giving us the
residuals] and `summarise`:

```{r}
augment(gm_lm) %>%
  summarise(
    RSS = sum(.resid^2),
    TSS = sum((lifeExp - mean(lifeExp))^2),
    R2 = 1 - RSS / TSS
  )
```

### Pattern searching in residuals

We want to look for patterns in residuals because of variance-bias
decomposition from equation \@ref(eq:bias-var-to). We have parts of errors that
are systematic (bias) and idiosyncratic (variance). In any given problem there
is inherent variability but we should be controlling for as much bias as
possible. We do this by *looking for pattern in the residuals* as systemic
errors can show up in residuals.

Let's plot the residuals to see if we notice anything:

```{r, eval=T}
augment(gm_lm) %>%
  ggplot(aes(gdpPercap, .resid)) +
  geom_point()
```

- This plot shows the residuals for the 
- This plot can show us some serious problems we can see that there is **one
  large cluster of points on the left side and a trend** on the left side of
  the plot which clearly has a downward sloping trend. The residual are
  becoming more negative as `gdpPercap` increases.

> REMINDER: residuals are the DIFFERENCE between the actual values, and the
> values our model spits out. I.e. the distance of the points from the line of
> best fit. Because our line is essentially a continuous set of points.

<!--chapter:end:05-simple-regression.Rmd-->

# History of least squares

## A constraint we can place when fitting a line {#og-constraint}

> This exercise shows us that the sum of the residuals is 0 $\iff$ the line
> passes through the point with the average value of `x` and the average value
> of $y$

Suppose $(x_i, y_i)$ are observations we wish to model as

\begin{align*}
y_{i} = \alpha + \beta x_{i} + \epsilon_{i}
\end{align*}

for some unknown optimal values of $(\alpha,\beta)$.  
For a given choice $(\hat{\alpha})$ let $\hat{y}_{i} = \hat{\alpha} +
\hat{\beta}x_{i}$ and $r_{i} = y_{i}  -  \hat{y}_{i}$

**Exercise/constraint**: Show that $\bar r = 0$ if and only if the line $y =
\hat \alpha + \hat \beta x$ passes through the point $(\bar x, \bar y)$.

**Problem**: There are (uncountably) infinitely many solutions with "zero
average error"

For a given $x \neq \bar x$, could predict *any* $y$ with one of these lines.
Any line that passes through this point satisfies this particular constraint
and can predict any value of $y$ given a value of $x$. This is not what we
want.

## Constraints and constrained methods

If the method is mathematically well-defined, producing a unique solution, then
theories formed using that method can be severely tested^[We want a well
defined problem so we can produce a single solution. This is why flexible
models are more difficult to disprove/say there is something absolutely
incorrect with the model].

So we will add some additional constraints to our original constraint of
passing through $(\overline{x},\overline{y})$:

1. The errors sum to 0

### Why squared errors

Of all the alternative measures you could try and minimise, the least squares
is the **most simple**. The other minimisations would lead to very difficult
computations. It is almost tradition to use least squares because it was the
only one remotely computable by hand back in the day.

Another reason we like to use least squares is because it has convenient
geometry.

We want to minimise the residuals because we want to show that the sum of the
residuals is 0 (from the [original constraint](#og-constraint)). We know from
\@ref(eq:regression-intercept) that we can rearrange for the residuals in terms
of $\beta_{0}$^[Here, and often, denoted as $\alpha$], $\beta_{1}$, and $y$.
So, at a minimum of^[This is an objective function as a function of the
intercept and slope that we are going to fit. pretty sure the $l$ is merely
notation name like calling a function $f$]:

\begin{align}
  \ell (\hat{\alpha}, \hat{\beta}) = \sum_i (y_i - \hat{\alpha} - \hat{\beta}
  x_i)^2 (\#eq:min-squared-errors)
\end{align}

Taking the partial derivatives^[A reminder that we set the variable that isn't
in the denominator of the differentiation notation to 0]:

\begin{align*}
  0&= \frac{\partial \ell}{\partial \alpha} = 2 \sum_i r_i\\
  0&= \frac{\partial\ell}{\partial\beta} = 2 \sum_i x_i r_i
\end{align*}

- We see that we have that the sum of residuals is equal to 0
- We thus have **orthogonality**

(**Orthogonality, uncorrelatedness, bias**). Since $\overline{r} = 0$ and $\sum
x_{i}r_{i} = 0$, we also have:

\begin{align*}
  \text{cor}(x,y) = 0
\end{align*}

- Correlation measures ***linear independence***. 
- If we minimised a different loss function^[The name of a function that
  calculates residuals, i.e. the difference between the output of our algorithm
  when given the real data point and the expected output (where the data point
  actually is)] and the resulting residuals were correlated with $x$, this
  would mean there is some remaining (linear) signal, i.e. we still have some
  bias

> Minimising the squared error rules out the systematic bias. As long as we
> minimise the squared error, we will be effectively eliminating bias.

## Risk: probability and loss

### Randomness and probability models

So far we have minimised the squared error on *observed data* (equation
\@ref(eq:min-squared-errors)). If we can introduce a probability model to our
data, some joint distribution $p_{X,Y}(x,y)$ then we can minimise:

\begin{equation}
  \min \mathbb{E}[(Y - \alpha - \beta X)^{2}] (\#eq:ese)
\end{equation}

i.e. minimise the expected squared error because we have made assumptions about
the probability distribution.

### Generative ML^[We will mainly focus on these in this course]

- Models that use probability distributions in machine learning are sometimes
  known as "generative" models beacause they:
  - Model the "data generation pricess" (DGP)
  - Can be used to generate synthetic data

### Conditional distributions

Supervised learning is mostly about modelling the conditional distribution of
the given outcome variable with the feature variables:

\begin{align*}
  p_{Y|X}(y|x) =  p_{X,Y}(x,y) / p_{X}(x)
\end{align*}

Some ML methods try to learn the entire distribution and others focus on a
summary like conditional expectation/conditional quantile:

\begin{align}
  &\text{ conditional expectation } = \mathbb{E}_{Y|X}[Y|X] (\#eq:cef)\\
  &\text{ conditional quantile } = Q_{Y|X}(\pi) \notag
\end{align}

You can visualise conditional distribution on the actual data plot using curves
showing $p_{Y|X}(y|x)$ at two values of $x$ by fixing some value on the
horizontal axis (i.e. this is what is given) and then looking at a probability
distribution above the point on the line of the vertical axis (figure
\@ref(fig:visual-conditional-prob)).

```{r visual-conditional-prob, echo=F, eval=T}
#| fig.cap='Conditional probability visualisation',
#| out.height="30%",
#| fig.align="center",
knitr::include_graphics(
  "./assets/06-history-ls-risk/img/2022-01-24-18-19-01.png"
)
```

- We can also see the difference between trying to understand the whole
  probability distribution vs just trying to compute a conditional
  distribution. I.e. the regression line gives us one number for any $x$^[i.e.
  $\mathbb{E}_{Y|X}[Y|X]$] but not the distribution around that number, i.e.
  the entire conditional probability distribution.

### Conditional expectation and minimisation of expected squared loss

It can be shown that the conditional expectation function (CEF, \@ref(eq:cef))
minimises the expected squared loss:

\begin{align*}
  f(x) = \text{arg}\underset{g}\min \mathbb{E}_{X,Y}\{ [ Y - g(X) ]^2 \}
\end{align*}

- So, out of all possible functions of $x$^[Provided that it is measurable and
  integerable], the one that **square error in predicting $y$** is the
  conditional expectation of $y$ conditional on $x$.

Similarly, **quantile regression** on the 50% quantile, that loss function is
just the absolute of the loss function:

\begin{align*}
  Q_{Y|X}(0.5) = \arg \min_g \mathbb E_{X,Y} [ |Y - g(X)| ]
\end{align*}

> For other quantiles you must take the absolute loss function and you have to
> tilt it

### Risk `==` expected loss

As see from the examples in the previous section, for a given **loss function**
$L(x,y,g)$, you can find the optimal regression function $f(x)$ by finding the
regression function whihc minimises the risk i.e.:

\begin{align*}
  R(g) = \mathbb{E}_{X,Y}[L(X,Y,g)]\\
  f(x) = \text{ arg }\underset{g}\min R(g)
\end{align*}

- Where $g$ is a function which predicts the value of $y$ from $x$
- The risk of a given function that predicts $y$ from $x$ is its expected loss

In statistical machine learning we can use algorithms like the Law of Large
Numbers, Central Limit Theorem, subsampling. We can do this because
expectations in probability models are the same thing as sums of data sampled
from the probability models:

\begin{align*}
  \mathbb E \longleftrightarrow \dfrac{1}{n}\sum
\end{align*}

### Additional modeling assumptions

We are currently looking at models which assume that the output variable is a
linear function of $x$ or that it can be approximated as a linear function of
$x$:

\begin{align*}
  f(x) := \mathbb E_{Y|X}(Y|X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p
\end{align*}

> There are no error terms $\epsilon$ in this equation is because this is a
> definition with no estimations and represents the absolute perfect model. In
> reality, there is always going to be some incorrectness in our models in
> general which are corrected by the error term we usually have.

<!--chapter:end:06-history-ls-risk.Rmd-->

# Multiple regression

Sometimes there are more than one variables that correlate at different
strengths with the output variable.

- For example, if we correlate government welfare with poverty, there may me
  another variable which dictates how much welfare people have to take, i.e.
  age; older people can't work as much so they may need more welfare. It can be
  more than just the fact that people are poor which makes them need welfare
- We need a way to see if the correlation between the $y$ variable is
  attributable to more than one $x$ variable.
- Multiple regression was developed because, if you think about physical
  experiments, you can isolate variables there and come to understand the
  relationship. If you cannot isolate variables it is difficult to understand
  their influence

## When $p$ > 1

When the number of predictor^[dependent] variables $p$ is greater than 1, we
now need to think of our regression not as a line of best fit but instead as a
hyperplane of best fit because we have multiple dependents now (figure
\@ref(fig:multi-reg-plane)).

```{r multi-reg-plane, echo=F, eval=T}
#| fig.cap='multi-reg-plane',
#| out.height="30%",
#| fig.align="center",
knitr::include_graphics(
  "./assets/07-multiple-linear-regression/img/2022-01-24-19-22-42.png"
)
```

But we are still doing a **least squares regression** so we are choosing the
plane which minimises residuals among all possible hyperplanes.

## Multi linear regression notation

There are various ways to write the formula for a multi regression. The first
is the formula for one observation $i$:

\begin{equation}
  y_{i} = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \dots + \beta_{i}x_{i}
  + \epsilon_{1} (\#eq:multi-regression)
\end{equation}

or the second method which uses a column of vectors and its inner product with
the coefficients:

\begin{align*}
  y_{i} = x_{i}^{T}\beta + \epsilon_{i}
\end{align*}

Or, thirdly we can use what is called "random variable notation" where we
denote all the observations in the set with capital letters:

\begin{align*}
  Y = X^{T}\beta + \epsilon
\end{align*}

And finally, we can also write it out all out once as a matrix which multiplies
together to give all the terms possible^[Two common practices with matrix
notation is to include a column of 1s for the intercept in the observations so
that each observaiton has a $1\times\beta_0$ term. Or alternatively is is
omitted by assuming the $\mathbf{y}$ vector is already centerd, i.e the model's
mean (where the regression line passes through) is 0 so there is no intercept]:

\begin{align*}
  \begin{pmatrix}
    y_1 \\
    y_2 \\
    \vdots \\
    y_n
  \end{pmatrix}
    =
    \begin{pmatrix}
    1 & x_{11} & x_{12} & \cdots & x_{1p}\\
    1 & x_{21} & x_{22} & \cdots & x_{2p}\\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & x_{n1} & x_{n2} & \cdots & x_{np}\\
  \end{pmatrix}
    \begin{pmatrix}
    \beta_0 \\
    \beta_1 \\
    \vdots \\
    \beta_p
  \end{pmatrix}
    +
    \begin{pmatrix}
    \varepsilon_1 \\
    \varepsilon_2 \\
    \vdots \\
    \varepsilon_n
  \end{pmatrix}
\end{align*}

or:

\begin{align*}
\mathbf{y} = \mathbf{X} \beta + \mathbf{\varepsilon}
\end{align*}

### Other notational conventions

We'll use common conventions in this course:

- **Bold for vectors, bold and upper case for matrices**
- Otherwise upper case *denotes random variable*
- Error terms $\varepsilon = y - \mathbf x^T \beta$ never truly observed
- **Residuals $r = y - \mathbf x^T \hat \beta$ used as a proxy for errors**
- Greek letters like $\beta, \theta, \sigma, \Sigma$ usually *unknown parameters*
- Greek letters with hats like $\hat \beta$ are estimates computed from data
- Roman letters that usually denote functions with hats, like $\hat f$ are also estimates
- Other Roman letters with hats like $\hat y$ are predictions

<!--chapter:end:07-multiple-linear-regression.Rmd-->

## Multiple linear regression and pseudoinversion

### Matrices and vectors in `R`

Detailed in section \@ref(matrices-and-vectors)

## Why does [pseudoinversion][pseudoinversion] work

Let's understand how pseudoinversion works.  
Let $\mathbf A^\dagger = (\mathbf A^T\mathbf A)^{-1} \mathbf A^T$, then

\begin{align*}
  \mathbf A^\dagger \mathbf A = (\mathbf A^T\mathbf A)^{-1} \mathbf A^T \mathbf
  A = (\mathbf A^T\mathbf A)^{-1} (\mathbf A^T \mathbf A) = \mathbf I
\end{align*}

## Least-squares solutions in matrix notation

We can write a $p = n$^[Any number of parameter variables] regression using
matrix notation as follows:

\begin{align}
  \hat {\mathbf \beta} = (\mathbf X^T\mathbf X)^{-1}\mathbf X^T \mathbf y =
  \mathbf X^\dagger \mathbf y (\#eq:matrix-multi-reg)
\end{align}

\@ref(eq:matrix-multi-reg) is a general formula which will work as long as
$\mathbf{X}^{T}\mathbf{X}$ is invertable^[I.e. the **columns of X have full
rank** (because the columns are our predictor variables) i.e. you can do a full
gauss reduction?]. Our matrices often are invertible if $n>p$^[observations are
greater than the number of variables].

## Linear algebra and geometric intuition

Okay so we have our model in matrix notation. Now we can get the predictions
form the linear model by

\begin{align*}
  \hat{\mathbf y} = \mathbf {X} \hat{\mathbf \beta} = \mathbf X (\mathbf
  X^T\mathbf X)^{-1}\mathbf X^T \mathbf y = \mathbf H \mathbf{y}
\end{align*}

if we define

\begin{align*}
  \mathbf H = \mathbf X (\mathbf X^T\mathbf X)^{-1}\mathbf X^T
\end{align*}

- We multiply the estimated betas by the observations of **X**
- The $\mathbf{H}$ matrix is called the "hat" matrix

$\mathbf{H}$ is also a projection matrix. If you square it, nothing happens:
$\mathbf{H}^{2} = \mathbf{H}$. When you have a projection matrix like this. The
matrix is the projection onto the column space of $\mathbf{X}$:

- For any $n$-vector **v**, th $n$-vector $\mathbf{Hv}$ is the *orthogonal
  projection of* **v** onto the column space of $\mathbf{X}$
- Because it is an orthogonal projection we implicitly minimise the least
  squares. I.e. among all the vectors you can form using the predictor
  varaibles, the orthogonal projection of vector $\mathbf{y}$ onto the
  columnspace of $\mathbf{X}$ gives us the plane which is closest to the
  actual result varaible

Question: why does the hat matrix project orthogonally

### The matrix loss function

We thus have the loss function

\begin{align*}
  L(\mathbf X, \mathbf y, \mathbf \beta) = (\mathbf y - \mathbf X
  \beta)^T(\mathbf y - \mathbf X \beta)$$
\end{align*}

(just a different way of writing sum of squared errors)

- Consider each coordinate separately and take univariate partial derivatives
- Use vector calculus and compute the gradient
- (Or even use matrix calculus identities)

Reach the same conclusion: at a stationary point of $L$,

\begin{align*}
\mathbf X^T \mathbf X \hat \beta = \mathbf X^T \mathbf y
\end{align*}

## Categorical predictors

Let's take the case where we have a categorical variable as a predictor. First
we create a distribution of observations from a sample and then create a factor
aout of them:

```{r, eval=T}
x <- as.factor(
  sample(
    c("A", "B", "C"),
    size = 10,
    replace = T
  )
)
```

- `sample` takes a vector of values we wish to choose/sample from, `size` is
  how large we want our sample and then `replace` specifies with our without
  replacement
- The `factor` function creates a categorical variable with our sample

`R` has a function called `model.matrix` which displays your model in matrix
from when you pass through a `~` formula:

```{r, eval=T}
print(model.matrix(~x))
X <- model.matrix(~ x - 1)
```

- Reminder: you only need to code $n - 1$ of the values in your categorical
  variable because the last one is implicit. In this case `A` is implicit and
  when the value is `B` or `C`, then it uses that intercept plus the offset of
  the variable
- The `- 1` in our formula means that we omit the intercept and code `A`
  instead

**Computing the hat matrix**:

```{r, eval=T}
# timxing X by its inverse (pseudo) to get the identity mat
Xinv <- X %*% ginv(X)
head(round(Xinv, 2))
```

To see how this matrix gives us predictions, let's look at which indexes
of our list of A, B, C `x` contained the letter C:

```{r, eval=T}
cat_positions <- which(x == "C")
cat_positions
```

These tells us what rows had C in matrix `X`. Nothing the output, let's look at
the corresponding columns to the output numbers. We note that the corresponding
columns are all the same:

```{r, eval=T}
Xinv[, cat_positions]
```

We notes that these columns are all the same. Furthermore, we note that C
appears `r length(cat_positions)` and the decimals present in the columns which
correspond to the rows who are the probability that a C will be in that
position. So what this matrix does to give predictions is that it gives the
within-group averages of the outcome variable. So *if we had the outcome vector
$\mathbf{y}$*, the **prediction for some $y$** that corresponds an observation
where the categorical variable's value is C, that prediction will be the
average $y$ among the rows of data that had the value of C.

## Interoperating coefficients of a regression

Because we have more variables, we can plot pairs of variables (residuals?)
together (similar to looking at the correlation matrix for all the variables)
which may reveal a higher dimensional relationship in the residuals (bias).

What people really want when inerpreting coefficients is:

1. Our regression model is a conditional expectation model (given $x$ what is
   $y$). People want interpreting coefficients to be as simple as taking the
   partial derivative of that predictor variable^[I.e. what is the impact
   ceterus paribus]:
   \begin{align*}
     \frac{\partial}{\partial x_j} \mathbb E[\mathbf y | \mathbf X] = \beta_j
     \approx \hat \beta_j
   \end{align*}
   When you partially differential a linear regression you are only left with
   one $\beta$ term
2. People want to believe that if you could feasibly change the variable
   associated with $\beta_{j}$ ($x_{j}$) then there would be a corresponding
   change in $y$ of $\beta_{j}$ units (i.e. a causal relationship)

Both of these desires can lead to serious incorrectness in our model. There
could be **relationships between predictors**^[i.e. predictors that covary].
There are also likely to be important variables which are not even in our
model.

::: {.example #non-linear-violation name="Breaking the assumption of nonlinearity"}
<br />\hfill\break
Suppose there is one predictor variable $x$, and a non-linear model fits the
CEF^[Conditional expectation function]:

\begin{align*}
  \mathbb E[\mathbf y |X = x] = \beta_0 + \beta_1 x + \beta_2 x^2
\end{align*}

We don't know the $\beta$'s but we have some data, and we use multiple linear
regression to fit the coefficients:

```{r eval = FALSE}
x2 <- x^2
lm(y ~ x + x2)
```

But, there's a **problem**:

\begin{equation}
  \frac{\partial}{\partial x} \mathbb E[\mathbf y | x] = \beta_1 + 2\beta_2 x
  \neq \beta_1 \approx \hat \beta_1
\end{equation}

- I.e. we cant tale the partial derivate because the 2nd term will still depend
  on $x$. This can be somewhat solved in simple scenarios where it's easy to
  see what other terms are dependent on $x$ by **only using the partial
  locally** (almost like you would with a Taylor approximation)

:::

---



<!--chapter:end:08-multiple-reg-r.Rmd-->


<!--chapter:end:09-topic.Rmd-->

# Quesitons

- Where the RSS is the sum of the squared differences between our model fit
  line and the points? \@ref(eq:rse)
- What the hell is going on in \@ref(why-squared-errors). In terms of
  orthogonality
- ### Risk `==` expected loss am confucsed about this section
- Why does the matrix only work on one side. I clearly need a matrix refresher
  ()

<!--chapter:end:98-questions-to-ask.Rmd-->

# `R` notes^[These are probably temporary and will go into own book]

> This is the more general stuff that isn't categorisable into the lecture
> work. For specific `R` functions just look at the coursework in the root
> directory of the course

## Using the `R` repl

To use the `R` repl, you just

```{bash}
R
```

type `R` on the command line. This launches the `R` repl and you can write any
code you want. I.e. when you want to install a package, you can use the repl

## Managing packages

To install a package in `R`, you use the install object:

```{r}
install.packages({
  package:string
})

# For example installing renv for virtual environments
install.packages("renv")
```

- You pass the name of your package as the argument as a string

## `R` virtual environments

You don't want global packages everywhere, some of them will only be useful for
particular projects. Queue `renv`, the `R` virtual environment manager.

To create a virtualenv, go into the root of your project directory and run

```{r create-venv}
renv::init()
```

to initialise your virtual environment.

- You should create your `.gitignore .Rbuildignore`, and `.Rprofile` files
  before running this because it auto updates them

You can then install packages as usual and once you have completed your
project, you can use `renv::snapshow()` to write the dependencies to the
`renv.lcok` file.^[Note that this only writes the libraries you call in your
code. So without a project, you will not have any dependencies]

### LSP in virtual environments

Just spent ages trying to figure out why my LSP wasn't picking up my
virtualenv. It was because the `R` package `languageserver` must be installed
in the virtualenv in order for neovim to find the script to execute.

LSP has:

- Formatting
- Completion

## Loading with the library command

Note: in \@ref(r-virtual-environments) we preface the `init()` command with `renv::` denoting that we want
to use the `init()` command from that specific package. If we were running a
command constantly, we could use:

```{r}
library(renv)
init()
```

to **bring all the function from the `renv` package into our environment's
scope**.

## Environemnt variables

You can get and set environemnt variables with:

```{r}
Sys.getenv()
Sys.setenv()
```

To remove an envrionement variable you can use:

```{r}
Sys.unsetenv()
```

## Viewing `R` documentation

In `R`, if you want any documentation, you just need to preface the command
with a question mark:

```{r}
?install
```

- Note that we don't call the function when we ask for help

## The `select` function

You can use the `select` function to select/deselect rows from your model:

```{r}
subset <- select(my_data, -"name of column")
```

## Indexing in `R`

Indexing in `R` is pretty similar to something like `pandas`. If I have a
dataset I can index it in one of two way. The first:

```{r}
myplot[5, ]
myplot[, 5]
myplot[2, 5]
myplot[2:4, 5:7]
myplot[myplot[, 5] == 7, 5]
```

- Line 1 returns row 5 and all columns
- Line 2 returns column 5 with all rows
- Line 3 returns cell from column 5 row 2
- Line 4 returns the cells in rows 2 to 4 and columns 5 to 7^[Not sure if it's
  inclusive or not]
- Line 5 return the rows in column 5 which have a value of 7

> Data indexing in `R` is always [<rows>, <cols>]

The second way is with the `$` operator:

```{r}
mydata[mydata$year == 2020, ]
```

- This will return all columns for the rows that contain 2020

### The `tidyverse` way of indexing

This is quite a javascript way of doing things. We use the `filter` function,
much like you would use it in js

```{r}
filter(dataset, c(year == 2000, continent == "Asia", ...))
```

## Miscellaneous tips

### Viewing your dataset in a spreadsheet-like manner

```{r}
View(<dataset>)
```

### Generating a polynomial

You can generate a polynomical using:

```{r}
poly({datacolumn}, {polynomial degree})
```

- This will basically plot a standard polynomial for the `x` data values
  (vector?) that you pass as an arg.

### Adding cols onto a dataframe

You can use `augment` to add columns onto a dataframe

```{r}
augment(my_df, newdata = newdf)
```

### Including all variables in your model

The shorthand to inlcude all the variables in your dataset in your model is:

```{r}
lm(y ~ ., data = my_data)
```

What you need to avoid here though is using a variable which is unique to every
piece of data, like brand, etc.

## Matrices and vectors

In `R` you can create a matrix using the `matrix` function:

```{r, eval=T}
x <- matrix(data = 1:9, nrow = 3, ncol = 3)
matrix(1:9, nrow = 3, ncol = 3, byrow = T)
```

- We see here that we specify the data as a vector^[This is kind of like a list
  but it only holds one type of data type and is strictly one dimentional] of
  numbers
- We also specify the number of rows and columns and the way in which the data
  is spread across the columns/rows


### Matrix multiplication

In `R`, you can multiply matrices together with the `%*%` operator. We can
check the dimensions of our matrix:

```{r, eval=T}
dim(x)
```

> **NB**: your matrices MUST be able to be mathematically multipliable of
> course (cols `==` rows). You must be careful when multiplying because `R`
> will not throw an error message if one of the dimentions are a multiple of
> the other

One of the **frustrating things** about `R` is that if you use the `dim`
function on a list object, it will say that it has no dimensions, but if you
use it in a function that treated it as a vector^[Vectors have to have defined
dimensions], it will have $0\times 0$ dimensoins:

```{r, eval=T}
dim(rep(1, 3))
x %*% rep(1, 3) # The list auto converts to a vector here
```

- `rep` is how you define a list of the same number repeating themselves for
  the number of times you pass in as the second argument

### Transpose and symmetry

Recall: even if a matrix $\mathbf A$ is not square, both $\mathbf A^{T}\mathbf
A$ and $\mathbf A\mathbf A^{T}$ **are square and symmetric**^[A matrix
multiplied by it's transpose is always square]:

```{r, eval=T}
A <- matrix(c(rep(1, 4), 0, 0), nrow = 3)
t(A) %*% A
```

- This matrix is invertable but `A %*% t(A)` is not

### Pseudoinversion

```{r, eval=T}
MASS::ginv(A)
```

- This is a generalised inverted matrix which can act like a "one sided"
  inverse on matrix `A`

The generalised inverse only works when multiplying on the left to give the
$2\times 2$ identity matrix:

```{r, eval=T}
ginv(A) %*% A
```

<!--chapter:end:99-R.Rmd-->

