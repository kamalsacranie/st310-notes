---
title: "ST310 - Machine Learning"
author: "Kamal Sacranie"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: report
bibliography: []
biblio-style: apalike
link-citations: yes
geometry: a4paper,margin=3cm
subtitle: "Machine Learning course using R with Joshua Loftus"
---

```{r, setup, include=F}
knitr::opts_chunk$set(attr.source = ".numberLines", eval = F)
```

# Preface {-}

The main readings used in this course are:

- **ISLR** [Introduction to Statistical Learning](https://statlearning.com/) 
- **ESL** [Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/)
- **CASI** [Computer Age Statistical Inference](https://web.stanford.edu/~hastie/CASI/)
- **Mixtape** [Causal Inference: The Mixtape](https://mixtape.scunning.com/index.html)
- **R4DS** [R for Data Science](https://r4ds.had.co.nz/)

> In this course the textbook readings are required readings but the lecture
> video emphasise themes that complement the textbook

I Like this guy. I wish I had taken this course for real lol :(

## `R` package list

- `tidyverse`: just a bunch of commonly used packages
- `gapminder`: just some random datasets we can use

<!--chapter:end:index.Rmd-->

---
author: "Kamal Sacranie"
numbersections: true
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: bookdown::pdf_document2
---

# (PART) Week 1 {-}

```{r, include=F}
knitr::opts_chunk$set(echo = TRUE)
```

# Simple plotting with `ggplot`

What is the association between GDP and life-expectancy:

```{r, echo=FALSE}
library(gapminder)
library(tidyverse)
```

## Using `ggplot`

::: {.example #gdp-life-exp name="GDP vs life expectancy"}
<br />\hfill\break

::: {.minipage latex-data=""}
```{r yes, fig.cap="The cars data."}
ggplot(
  # Piping to mutate to boolean index the data
  gapminder %>% mutate(
    # Simple boolean logic here and passing it as a kwarg to the mutate
    # function
    indicator = (country == "United Kingdom")
  ),
  aes(x = gdpPercap, y = lifeExp)
) +
  # How the fuck has indicator now got local scope. Was it created by the
  # mutate fucntion. But even then? it's not returning anything, where is
  # idnicator being assigned
  geom_point(aes(color = indicator))
```
:::

:::

---

- `aes` stands for aesthetic and creates an aesthetic mapping between our
  dataset and the aesthetic properties of the plot. It's a function which is
  passed as an in place arg called a `mapping` function
- `aes` takes an x variable and y variable form our dataset as arguments
- `geom_point` is one of the many plots that come with `ggplot` and plots a
  point plot

> The `aes` argument which goes in the first `ggplot` function applier for the
> whole "canvas", whereas the ones for the secondary functions only apply to
> that specific function. So **if you want to apply colour to only the
> regression line, you do the `aes` in the `geom_smooth` function**

In the above example we use the `%>%` operator (provided by the package dyplr).

This is maybe the only good thing about R that I have encoutered. It works
similarly to the pipe character in bash. It takes the return from the last
function pipes it into the first argument of the next function.

> For the record, it is insanely retarded that we can just pass through the
> columns of `gapminder` as if they were variables. We haven't assigned them or
> anything. It's also insane to me that we use ggplot and then ADD A PLOT?? WTF
> does that even mean. Why are these two objects able to be added. How is there
> not a type error

## The pipe `%>%`

As previously mentioned... the only good thing about `R`. Just plugs the data
into the first argument of the next function.

::: {.example #pipe-vs-normal name="Piping to a function"}
<br />\hfill\break

```{r}
gapminder %>% nrow()
```

is the same as

```{r}
nrow(gapminder)
```

:::

---

This is often what `R` uses to stack things. It's basically becasue `R` is made
by people who are too retarded to read regular nested code or even just regular
lines of code.

## Filtering for a value in column (basically boolean indexing)

```{r}
# notice that we can pass a straight int here. R knows the type in the column
base_plot <- ggplot(
  # Easier to see here that mutate basically adds a column with a boolean value
  # showing whether it's the UK or not
  filter(gapminder, year == 2007) %>%
    mutate(indicator = (country == "United Kingdom")),
  aes(x = gdpPercap, y = lifeExp)
) + # The plus is similar to the pipe, it is introduces by ggplot and basically
  # allows you to build layers on your plot
  geom_point(aes(color = indicator))
```

- The `+` is like ggplot's version of the pipe. It's used to layer graphs
- `indicator` is accessable because it's a column of our `gapminder` data which
  is being pushed through with our plus

```{r}
base_plot +
  geom_smooth()
```

- The line curves around the points and fits locally to the points
- The gray area is the confidence intreval of the fit line

## Model objects

Everything is an object in `R` and our assignment operator is `<-`:

```{r}
gm_2007 <- gapminder %>%
  filter(year == 2007)
```

Great so we can use that everywhere in this `R` file. For example, we can use
the `lm` function for fitting models

::: {.example #lm-basic-fit name="A basic model function use"}
<br />\hfill\break

```{r}
gm_lm <- lm(lifeExp ~ gdpPercap, data = gm_2007)
```

:::

---

> A common way for model funciton to work in `R` is to require the assignment
> of the lefthand side and the right hand side variables; we see this in
> \@ref(exm:lm-basic-fit) with the use of the `~`. On the left of the tilde we
> have the dependent (outcome) variable and on the right we have the
> independent (predictor) variable

In \@ref(exm:lm-basic-fit) `lm` stands for linear model (a regression) and is
an object which contains all the information for our regression. When you print
it on its ones, there isn't much you can do

```{r}
summary(gm_lm)
residuals(gm_lm) # the difference in the predicted values and the actual
predict(gm_lm)
```

- `summary` is a builtin function which displays a summary **of any model
  object in `R`**
- This also shows us standard errors, p-values, etc.

> Something I find weird here is that all the data we need is inside the `lm`
> object but instead of using a method to produce the result like I would
> expect, we have to pass it to another function

## `broom`

`broom` is just a library that provides nice out of the box rendering for `R`'s
model objects.

```{r}
library(broom)
```

```{r}
tidy(gm_lm)
```

- The `broom::tidy` function is really nice for making tables look good
- Don't think it works in latex though

We also have the `broom::glance` function which shows our most important values
at a glance. You can also compare two models with it:

```{r}
glance(gm_lm)
```

The last main fucntion from `broom` is `augment`. This gives us the original
dataset back plus the extra rows like standard errors for residuals, residuals,
etc. This function is really useful for plotting the results of the model

```{r}
augment(gm_lm) %>%
  ggplot(aes(x = gdpPercap, y = lifeExp)) +
  geom_point() +
  # the geom line adds a line to the plot on top of the already existing plot,
  # i.e. we can give it a different x and y to plot via the aes which returns a
  # mapping
  geom_line(aes(x = gdpPercap, y = .fitted))
```

- Note here how ggplot basically crease the canvas for the plot. Then you layer
  on a point distribution using the data, then you layer on a line using the
  data and specifying the variable you want. In this case `.fitted` will give
  us all the points on the fitted line. This is the same as using ggplot's
  `geom_smooth` function. We are just doing it the long way around

## Subbing out function for e.g. `loess`

The most important takeaway here is the idea of **model flexibility**. We have
made a robust programme which let's us swap in and out functions. For example,
let's use the `loess` function to plot the local extimation line which is the
default given by ggplot's `geom_smooth`

```{r}
gm_loess <- loess(lifeExp ~ gdpPercap, data = gm_2007, span = .5)
summary(gm_loess)
```

- Again, we see the tilde notation for specifying LHS and RHS
- An interesting note. When I ran what I thought was known as the `tidy`
  **function**, `R` threw an error mentioning loess object doesn't have the
  method `tidy`. I.e. methods are defined in objects but somehow they are
  called with function notation.

::: {.example #loess name="loess example"}
<br />\hfill\break

```{r}
augment(gm_loess) %>%
  ggplot(aes(gdpPercap, lifeExp)) +
  geom_point() +
  geom_line(aes(gdpPercap, .fitted))
```

:::

---

### What is `loess` actaully doing

`loess` stands for "locally estimated scatter-plot smoothing". If we look at
the output of exmaple \@ref(exm:loess), we see that it is looking at the local
points and approximating them. `loess` has some options, for example:

- `span`: tells the function how localised to make its approximation

## Questions

- Still confused how we can access our column as variables

<!--chapter:end:01-introduction-to-r.Rmd-->

---
author: "Kamal Sacranie"
numbersections: true
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: bookdown::pdf_document2
---

# Candy rankings

```{r}
library(plotly)
library(broom)
library(fivethirtyeight)
library(tidyverse)
```

It's always good practice to start by visualising some of your data. For fun
let's just fit a simple linear model to this dataset:

```{r}
cr_lm <- lm(pricepercent ~ winpercent, data = candy_rankings)
tidy(cr_lm)
```

```{r}
augment(cr_lm) %>%
  ggplot(aes(x = pricepercent, y = winpercent)) +
  geom_point() +
  geom_line(aes(y = .fitted))
```

- Nice tip with `geom` functions is that because it take in the object the
  previous function returns, you can keep the `x` value the same and reassign
  the `y`

## Adding a categorical variable to our model

```{r}
cr_lm_cat <- lm(winpercent ~ pricepercent + chocolate, candy_rankings)
tidy(cr_lm_cat)
```

- Here in the `formula` argument, we add the categorical variable `chocolate`
- This becomes a dummy variable for our regression model (`chocolateTRUE`)

Let's now represent this new categorical variable on our plot through the use
of colour:

```{r}
augment(cr_lm_cat) %>%
  ggplot(aes(
    x = pricepercent,
    y = winpercent,
    color = chocolate,
    shape = chocolate,
    linetype = chocolate
  )) +
  geom_point() +
  geom_line(aes(y = .fitted))
```

- So in the `aes` function we have a whole load of arguments which take a set
  of true or false values to boolean index the characteristics of the data

## Adding a continuous predictor and creating a 3D plot

A common package used to create 3D plots is `plotly`. 

```{r}
candy3d <- plotly::plot_ly(candy_rankings,
  x = ~pricepercent, y = ~sugarpercent,
  z = ~winpercent,
  type = "scatter3d"
)
```

> This is a perfect example of how `R` is so good out the box for data
> visualisation

We can fit our regression plane in this three dimensional space:

```{r}
cr_lm_sugar <- lm(
  winpercent ~ pricepercent + sugarpercent,
  data = candy_rankings
)

xy_plane <- expand.grid(0:100, 0:100) / 100
ps_plane <- xy_plane %>%
  rename(pricepercent = Var1, sugarpercent = Var2)

lm_plane <- augment(cr_lm_sugar, newdata = ps_plane)
lm_matrix <- matrix(lm_plane$.fitted, nrow = 101, ncol = 101)

candy3d %>%
  add_surface(
    x = ~ (0:100) / 100,
    y = ~ (0:100) / 100,
    z = ~lm_matrix
  )
```

- The `expand.grid` function creates a data frame from all combinations of the
  supplied vectors or factors
- We then rename the variable of the plane with `rename`
- We then see the `augment` function in use 


<!--chapter:end:02-more-involved-models.Rmd-->

# Machine learning applications

Some examples of machine learning are:

- Scraping social-media to predict period of unrest before it happens
- Using health records to predict which patients will require more care
- Algorithms which collect data to adapt

Basically any predictive model that uses data to predict the future. In this
course we focus on the underlying theory of how the methods function.

> The difference between AI and ML, is that AI allows us to get mathematical
> relationships from data that is unstructure, non-mathematical. This could be
> images etc. Contrasting this with what we did in section
> \@ref(simple-plotting-with-ggplot) where our inputs were very mathematical
> and numerical. In this course **when we refer to data, we are already talking
> about vectorised, structured data**

## Abstraction and notation

In an abstract sense, the data we look at is formatted as a collection of $p$
district **variables**^[Think columns of your data]:

\begin{align*}
  X = (X_1, X_2, \ldots, X_p) \in \mathbb{R^{p}}
\end{align*}

- We assume each **observation is a point in the vectors space**
  $\mathbb{R^{p}}$
- We also assume that $p$ is finite

**Supervised learning**: if we think of an application for ML which has a clear
$Y$ variable defined, i.e. the outcomes^[Sometimes called responsive] of the
model, the we are dealing with supervised learning.

**Unsupervised learning**: if there is no natural outcome variable $Y$, then we
are dealing with unsupervised learning. This is where you try and create a
mathematical relationship out of unstructured data.

### Sub categories of supervised ML

Common cases:

- If $Y$ is numeric: **regression**
- If $Y$ is categorical: **classification**

Special cases:

- $Y$ is binary with rare cases, e.g. anomaly detection
- $Y$ is a time to event, survival analysis
- Multi-class, hierarchical classes, etc

## How to predict $Y$ from $X$

- We want to estimate $\exists f$ such that the graph of the function $y =
  f(x)$ fit the data perfectly^[Fundamental idea of regression]
- **Problem**: what if $(x_1, y_1) = (1, 0)$ and $(x_2, y_2) = (1, 1)$?
- **Problem**: even our most tested and verified physical laws won't fit data
  perfectly

Even if we are thinking about cases where our data is supposed to follow a
well-defined law, upon measuring we will often see that this doesn't quite
work.  
The **solution** to this problem is to have an error term. For any function
$f$, we can always get the residuals:

\begin{align*}
  \epsilon\equiv y - f(x)
\end{align*}

- $y$ is the actual observation and we deduct the predicted value to see our
  residual (error)

***We want a function which minimises error.***

The simplest way to minimise this error is to find out which function decreases
our squared error^[This is premised on the fact that $\epsilon$ has some sort
of probability distribution so we can predict it]. A good function is defined
as:

\begin{align*}
  \mathbb{E}[\epsilon^2] = \mathbb{E}\{[Y - f(X)]^2\}
\end{align*}

This motivates the **plug-in principle**: compute an *estimate* $\hat{f}$ of
the good function $f$ by solving the corresponding problem on the dataset,
i.e.:

\begin{align*}
  \min\sum_{i = 1}^{n}\left[y_{i} - \hat{f}(s_{i})\right]^{2}
\end{align*}

> Basically, there is this perfect function out there $f$. It is almost
> impossible to arrive at that function so we have to calculate $\hat{f}$ which
> minimises our error and approximates our perfect function.

## Bias-variance trade-off

Errors that are systematic are bias in the system. The bias-variance trade-off
is formally described as follows:

\begin{equation}
  \mathbb{E}\{[Y - \hat{f}(X)]^{2}\} = \text{Var}(\hat{f}) +
  \text{Bias}(\hat{f})^{2} (\#eq:bias-var-to)
\end{equation}

- The expected error of our function is a combination of bias and variance

Combining this idea of BV trad-off with our idea of model complexity, we
typically see that **more complex models have lower bias and higher variance**.

Typical there is a sweet spot for complexity

Figure \@ref(fig:complexity-variance) shows us a great preview between a more
complex model which visually even has more variance and a less complex model
which doesn't deviate too much but as such has more bias.

```{r copmplexity-variance, echo=FALSE, fig.cap='copmplexity-variance', out.height="30%", fig.align="center"}
knitr::include_graphics(
  c(
    "./assets/03-machine-learning-applications/img/2022-01-23-02-58-55.png",
    "./assets/03-machine-learning-applications/img/2022-01-23-02-59-17.png"
  )
)
```

- It is difficult to tell visually which model is better

## Evaluation: mean squared error

It is extremely easy to evaluate MSE in `R`. You use the `residuals` function
which takes a `model` object as an argument:

```{r, eval=T}
c(
  mean(residuals(gm_simple)^2),
  mean(residuals(gm_complex)^2)
)
```

`candy_rankings` models:

```{r, eval=T}
c(
  mean(residuals(candy_simple)^2),
  mean(residuals(candy_complex)^2)
)
```

- These variables we are passing through are object from a model function like
  `lm`
- The above results show us that the more complex models have lower MSE. This
  is mostly true because your function better approximates the data. But this
  is where the question of overfitting comes into play because if we use that
  same function on a new set of data, we could get completely wrong results

<!--chapter:end:03-machine-learning-applications.Rmd-->

# Seminar 1

This seminar is insanely basic. Probably nothing to write down.

<!--chapter:end:04-seminar-1.Rmd-->


<!--chapter:end:04-topic.Rmd-->


<!--chapter:end:05-topic.Rmd-->


<!--chapter:end:06-topic.Rmd-->


<!--chapter:end:07-topic.Rmd-->


<!--chapter:end:08-topic.Rmd-->


<!--chapter:end:09-topic.Rmd-->


<!--chapter:end:10-topic.Rmd-->

# `R` notes^[These are probably temporary and will go into own book]

> This is the more general stuff that isn't categorisable into the lecture
> work. For specific `R` functions just look at the coursework in the root
> directory of the course

## Using the `R` repl

To use the `R` repl, you just

```{bash}
R
```

type `R` on the command line. This launches the `R` repl and you can write any
code you want. I.e. when you want to install a package, you can use the repl

## Managing packages

To install a package in `R`, you use the install object:

```{r}
install.packages({
  package:string
})

# For example installing renv for virtual environments
install.packages("renv")
```

- You pass the name of your package as the argument as a string

## `R` virtual environments

You don't want global packages everywhere, some of them will only be useful for
particular projects. Queue `renv`, the `R` virtual environment manager.

To create a virtualenv, go into the root of your project directory and run

```{r create-venv}
renv::init()
```

to initialise your virtual environment.

- You should create your `.gitignore .Rbuildignore`, and `.Rprofile` files
  before running this because it auto updates them

You can then install packages as usual and once you have completed your
project, you can use `renv::snapshow()` to write the dependencies to the
`renv.lcok` file.^[Note that this only writes the libraries you call in your
code. So without a project, you will not have any dependencies]

### LSP in virtual environments

Just spent ages trying to figure out why my LSP wasn't picking up my
virtualenv. It was because the `R` package `languageserver` must be installed
in the virtualenv in order for neovim to find the script to execute.

LSP has:

- Formatting
- Completion

## Loading with the library command

Note: in \@ref(r-virtual-environments) we preface the `init()` command with `renv::` denoting that we want
to use the `init()` command from that specific package. If we were running a
command constantly, we could use:

```{r}
library(renv)
init()
```

to **bring all the function from the `renv` package into our environment's
scope**.

## Environemnt variables

You can get and set environemnt variables with:

```{r}
Sys.getenv()
Sys.setenv()
```

To remove an envrionement variable you can use:

```{r}
Sys.unsetenv()
```

## Viewing `R` documentation

In `R`, if you want any documentation, you just need to preface the command
with a question mark:

```{r}
?install
```

- Note that we don't call the function when we ask for help

## The `select` function

You can use the `select` function to select/deselect rows from your model:

```{r}
subset <- select(my_data, -"name of column")
```

## Indexing in `R`

Indexing in `R` is pretty similar to something like `pandas`. If I have a
dataset I can index it in one of two way. The first:

```{r}
myplot[5, ]
myplot[, 5]
myplot[2, 5]
myplot[2:4, 5:7]
myplot[myplot[, 5] == 7, 5]
```

- Line 1 returns row 5 and all columns
- Line 2 returns column 5 with all rows
- Line 3 returns cell from column 5 row 2
- Line 4 returns the cells in rows 2 to 4 and columns 5 to 7^[Not sure if it's
  inclusive or not]
- Line 5 return the rows in column 5 which have a value of 7

> Data indexing in `R` is always [<rows>, <cols>]

The second way is with the `$` operator:

```{r}
mydata[mydata$year == 2020, ]
```

- This will return all columns for the rows that contain 2020

### The `tidyverse` way of indexing

This is quite a javascript way of doing things. We use the `filter` function,
much like you would use it in js

```{r}
filter(dataset, c(year == 2000, continent == "Asia", ...))
```

## Miscellaneous tips

### Viewing your dataset in a spreadsheet-like manner

```{r}
View(<dataset>)
```

### Generating a polynomial

You can generate a polynomical using:

```{r}
poly({datacolumn}, {polynomial degree})
```

- This will basically plot a standard polynomial for the `x` data values
  (vector?) that you pass as an arg.

### Adding cols onto a dataframe

You can use `augment` to add columns onto a dataframe

```{r}
augment(my_df, newdata = newdf)
```

### Including all variables in your model

The shorthand to inlcude all the variables in your dataset in your model is:

```{r}
lm(y ~ ., data = my_data)
```

What you need to avoid here though is using a variable which is unique to every
piece of data, like brand, etc.

<!--chapter:end:R.Rmd-->

